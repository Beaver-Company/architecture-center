# Batch processing for IoT using HDInsight and Hive


This chapter describes using Azure Functions to perform warm-path processing in an IoT solution. As described in the [Introduction](./index.md) to this series, the Drone Delivery application has the following functional requirements for warm-path processing:

The cold path performs batch processing, to get insights over longer time periods (hours, days, month).
The cold path should capture the raw device telemetry with as little processing as possible. This enables you to run new queries over your historical data, to get new insights. If you filter or aggregate the data before capturing it, then you are losing information.
When you create a new query, you will run it over your complete data set. As the data set continues to grow, this will take longer.
Often, you can perform incremental processing, by running the job on a schedule and processing just the new data. You'll need to consider how to merge the data sets, and what happens at the boundaries. For example, in our scenario, we are processing deliveries and looking for start and end times. If you run the job daily on the previous day's data, what happens if a delivery happens just before midnight and spans two days? 

Our workflow:
In the IoT Hub, add storage as a custom endpoint. (See here) – The route query is “true” because we want to get all of the telemetry, and not filter.
The data is written to blob storage in Avro format. It is written in batches and stored in containers grouped by partition and date (year, month, day, hour, minute).  Hive can read Avro format.

First step in Hive is to define external tables for the data. An external table is a table where the data resides outside of outside of Hive (in this case, blob). The table metadata is stored in Hive.

```sql
CREATE EXTERNAL TABLE rawtelemetry
  ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
  STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
  OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
  LOCATION
  'wasbs://telemetry@<your storage account>.blob.core.windows.net/<your IoT Hub name>/'
  TBLPROPERTIES (
    'avro.schema.url'='wasbs://telemetry@<your storage account>.blob.core.windows.net/telemetry.avsc');
```

The table schema is defined by the avsc file which is generated by running Avro tools against a sample data file. 
The next step is a Hive query that loads the data from external tables into Hive. This steps also converts the external tables into a format that is easier for processing.

```sql
INSERT INTO TABLE enrichedtelemetrytable
SELECT * FROM (
    SELECT
        get_json_object(telemetryjson,'$.occurrenceUtcTime') AS occurrenceUtcTime,
        get_json_object(telemetryjson,'$.deviceId') AS deviceId,	  
        get_json_object(telemetryjson,'$.sensorType') AS sensorType,
        get_json_object(telemetryjson,'$.deliveryId') AS deliveryId,
        get_json_object(telemetryjson,'$.velocity') AS velocity,
        get_json_object(telemetryjson,'$.acceleration') AS acceleration,
        split(get_json_object(telemetryjson,'$.position'),'\\|')[0] AS latitude,
        split(get_json_object(telemetryjson,'$.position'),'\\|')[1] AS longitude,
        split(get_json_object(telemetryjson,'$.position'),'\\|')[2] AS altitude,
        array('DPS', 'INF', 'PUP', 'DOP', 'ARS')[cast(get_json_object(telemetryjson,'$.flightStatus') AS int)] AS flightStatus
	    --Flight Status Codes: 1) Depart station, 2) In flight, 3) Picked up package, 4) Dropped off package, 5) Arrived at station
    FROM (
	    SELECT cast(body as string) AS telemetryjson FROM rawtelemetry
    ) sub1
) sub2
WHERE sensorType LIKE 'drone-event-sensor%'
```

This query has three nested SELECT statements
-	The inner SELECT statement takes the body from the Avro message as a string. (Avro is natively a binary format, hence the CAST statement)

SELECT cast(body as string) AS telemetryjson FROM rawtelemetry

-	The next SELECT statement converts the raw JSON string into a shape:
o	Splits the JSON body into separate columns
o	Splits the JSON “position” field into separate latitude, longitude, and altitude values 
o	Converts the JSON “flightStatus” field from an integer into a value that is more meaningful to the backend business systems, which use three-letter codes for flight status.
o	
-	The outer SELECT contains the WHERE clause that selects only drone “event” sensor type (which contains position data), filtering out the "state” sensor type that contains temperature, battery, and other drone engine state.

The final HIVE query finds matching start/stop events for each deilvery, in order to calculate the delivery time for each delivery.

```sql
INSERT INTO TABLE deliverysummarytable
SELECT deliveryId, max(startDateTime) as startDate, max(completeDateTime) as completeDate
FROM (
    SELECT deliveryId,
        CASE
            WHEN flightStatus == 'DPS' --Departed Station
                THEN occurrenceUtcTime
                ELSE null
            END AS startDateTime,
        CASE
            WHEN flightStatus == 'DOP' --Dropped off Package
                THEN occurrenceUtcTime
                ELSE null
            END AS completeDateTime
    FROM enrichedtelemetrytable
    WHERE flightStatus == 'DPS' OR flightStatus == 'DOP'
) sub
GROUP BY deliveryId
```
Every flight event has an occurrenceUtcTime field, which is the time stamp of the event. This query projects occurrenceUtcTime either to startDateTime or to, completeDataTime, depending on whether the flight status for the event is ‘DPS' (departed station) or ‘DOP' (dropped off package).

Enriched telemetry table

| occurrenceutctime	| deviceid | sensortype | deliveryid | velocity | acceleration | latitude | longitude | altitude | flightstatus |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 6/7/2018 2:32:29 PM | drone-01.4467 | drone-event-sensor;v1 | 00a8907c-6e9c-495a-af12-99f315df46ee | 58.69 | 2.49 | 47.603006 | -122.004185 | 494.99 | DPS |
| 6/7/2018 2:32:34 + PM | drone-01.4467 | drone-event-sensor;v1 | 00a8907c-6e9c-495a-af12-99f315df46ee | 59.3 | 2.5 | 47.603008 | 122.00414 | 494.98 | INF |
| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |
| 6/7/2018 2:42:44 PM | drone-01.4467 | drone-event-sensor;v1 | 00a8907c-6e9c-495a-af12-99f315df46ee | 60.3 | 2.5 | 47.678758 | -121.891975 | 494.99 | DOP |

After the inner SELECT:

| deliveryid | startdate | completedate |
|------------|-----------|--------------|
| 00a8907c-6e9c-495a-af12-99f315df46ee | 6/7/2018 2:32:29 PM | null |
| 00a8907c-6e9c-495a-af12-99f315df46ee | null | 6/7/2018 2:42:44 PM |

After the outer SELECT / GROUP BY:

| deliveryid | startdate | completedate |
|------------|-----------|--------------|
| 00a8907c-6e9c-495a-af12-99f315df46ee | 6/7/2018 2:32:29 PM | 6/7/2018 2:42:44 PM |

At this point it's possible to load the table into Excel or Power BI for analysis. However, the Hive tables can only be read while the HDInsights cluster is running. Typically, you would copy the output of the analysis (the last query in our example) into another data store such as CosmosDB, to be used by a serving layer.
For storing the raw telemetry data, we recommend using General-purpose v2 storage. Pricing for GPv2 accounts has been designed to deliver the lowest per gigabyte prices. In addition, GPv2 storage supports hot, cool, and archive tiers.
Consider the following tiering strategy:
-	Use hot tier to store incoming telemetry for batch processing. 
-	After a period of time (say, 1 month), move a blob to cool tier. Cool storage offers similar latency (milliseconds) to hot storage, but the cost it optimized for storage versus data retrieval, so it's intended for longer-term storage of data that is less frequently accessed.
-	Very old data (say, 3 years) can be moved to archive storage. Archive storage is much less expensive, but has a retrieval time of several hours, so don't use this tier if you expect to run on-demand queries over the data.
Note that changing tiers incurs a one-time cost, so you shouldn't frequently switch a blob between tiers.
Your organization may have retention policies for regulatory and compliance reasons. 
Another storage option to consider is Azure Data Lake Store, if you need any of the more advanced features that Data Lake Store offers such as POSIX ACLs to grant file-level permissions. See Comparing Azure Data Lake Store and Azure Blob Storage. However, also be aware that Data Lake Store is available in limited regions. 

When choosing a data store for raw telemetry, consider the following:
-	Storing raw telemetry requires high write throughput 
-	It is append-only, records are never updated after write.
-	Writing can be done in batches for efficiency.
-	Reads will be full scans, not singleton lookups.
-	The telemetry data should be partitioned by time intervals (for example, by hour) so that you can run incremental batch jobs over a specified time span.
-	Storage should be redundant to guard against possible data loss.
-	Must be compatible with your batch processing layer. 
Blob storage using IoT Routing meets all of these requirements.
